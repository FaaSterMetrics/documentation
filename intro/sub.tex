\documentclass[../main.tex]{subfiles}
\begin{document}
\section{Project Motivation}\label{sec:Intro}

Serverless paradigms such as Function-as-a-Service (FaaS) have recently gained considerable interest 
as they theoretically allow infinite scaling while promising low operational maintenance. 
The latter is achieved by a high-level abstraction of hardware, 
hence reducing the required code to almost pure business logic. 
Consequently, it comes to no surprise that many companies, such as AWS, Google, IBM, 
as well as various open source projects, have entered the field to contribute different FaaS platforms.

As more programs and business processes are getting implemented on FaaS platforms, 
the FaaS landscape nowadays also includes increasingly complex structured programs, 
which were previously generally only attributed to monolithic or microservice-based systems. 
However, these new complex FaaS applications and the performance of their platforms are not well understood at the time of writing. 
Current state of the art FaaS benchmarks are mainly focusing on single-function microbenchmarks. 
They can precisely evaluate specific aspects of a platform but lack the ability to measure how real-world applications perform. 
Furthermore, only a few FaaS benchmarks feature reproducibility and portability of tests thus far. 
Many benchmarks are bespoke for their test platform, sometimes not even open-source, 
and thus not portable to create comparisons to other platforms.

As real-world applications are more complex in size and connectedness than a collection of microbenchmarks, 
and can even be run across several FaaS platforms at the same time, 
a new comprehensive benchmark suite is needed to understand and compare FaaS platforms.

To resolve this issue, we introduce FaaSterMetrics: 
A comprehensive FaaS benchmarking framework that offers pre-defined real-world applications and 
test scenarios for comparing multi-provider setups. 
Moreover, this framework is designed to produce verifiable results and to be easily extendable 
so that e.g.\@ support for novel FaaS providers can be added without needing to restructure the whole project.

Our framework works without relying on platform-specific APIs 
by modifying the deployed program and injecting self-brought routines 
in order to measure execution time from within the program itself.

\subsection{General Accomplishments}%
\label{sub:introAccomplishments}

The main contributions of this work are:
\begin{enumerate}
  \item A benchmarking suite for FaaS-provider independent real-world applications, including a set of standard test scenarios.
  \item To be open source and featuring a straight-forward implementation, extendable to both novel and established FaaS providers.
  \item High-level analysis tools to work with data generated by the benchmark. 
  \item A usage guide and schematic overview of all parts of our framework.
\end{enumerate}
A more detailed list of our project's requirements and how we fulfilled them 
can be found in \Cref{tab:requirements} and \Cref{tab:fulfillments}.

\begin{longtable}{l l} 
  \caption[FaaS Benchmarking Suite Requirements]{Our FaaS Benchmarking Suite Requirements\vspace*{1mm}}\label{tab:requirements}\\
  \textbf{Requirement} & \textbf{Rationale}\\ 
  \toprule
  %\endhead{}%
  \multicolumn{2}{c}{\emph{\textbf{Functional Requirements}}}\\
  \midrule[0.06em]
  \makecell[{{p{4cm}}}]{%
  Application-Driven Benchmarking Framework
  }&
  \makecell[{{p{10cm}}}]{%
    One main motivation for this project is that all current FaaS benchmarks we know of
    are microbenchmark-focused and do not completely represent the setting of real-world applications.
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  Representative Demo Applications
  }&
  \makecell[{{p{10cm}}}]{%
    Application-driven benchmarking naturally relies on working test applications,
    representing common real-world scenarios.
    Both functional programs and automated workloads for them should accompany the benchmarking framework.
    Having pre-defined demo applications contributes to the framework's adoptibility as well,
    since they help understanding the framework's application design.
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  Support for Cross-Provider Setups
  }&
  \makecell[{{p{10cm}}}]{%
    When building complex real-world applications, it may be useful or needed to split
    functionality across multiple FaaS platforms.
    Additionally, our framework should also be able to evaluate those platforms themselves in terms of performance,
    given a specific scenario.
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  Easy Adaptibility to New Providers
  }&
  \makecell[{{p{10cm}}}]{%
    It should be possible to extend the framework, 
    such that support for novel and even experimental providers can later be added. %TODO achieved by e.g. common experiment process
    Especially in a fairly young environment such as the FaaS landscape,
    there may still emerge new platforms which are e.g.\@ specialized in a certain context.
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  Time Measurements %%% Note: Function and Calling
  }&
  \makecell[{{p{10cm}}}]{%
    In order to evaluate performance and to build the fundamental basis of benchmarking, time measurement is necessary. 
    We especially have to measure the time it takes to execute a function as well as 
    how long it takes to call other functions (which includes network delay). 
    This can also help us reasoning about hardware resources of a platform (e.g. CPU, I/O, RAM, network).
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  Request Tracing 
  }&
  \makecell[{{p{10cm}}}]{%
    More sophisticated programs will lead to at first glance opaque function interactions. 
    It is essential to be able to follow through each step of a program, 
    such that white box testing is made possible and bottlenecks can be spotted.
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  Coldstart Detection %%% 
  }&
  \makecell[{{p{10cm}}}]{%
    Even though the serverless FaaS features a lot of flexibility, 
    cold-starting of instances is a common downside of FaaS programs and 
    detecting cold-starts is thus a necessity to have included into our framework.
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  External Database Support
  }&
  \makecell[{{p{10cm}}}]{%
    Since FaaS programming is stateless by its paradigm,
    interacting with external databases that keep any state is 
    inevitable in many FaaS applications.
    This need has to be supported by our framework.
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  Data Export to Common Format (CSV) %%% 
  }&
  \makecell[{{p{10cm}}}]{%
    In order to understand an experiment, the generated measurement data has to be cleaned and prepared for later analysis. 
    As the most common format for exporting data CSV needs to be supported.
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  Simple Data Visualisation %%% Common interests like function time + call graph, technically optional
  }&
  \makecell[{{p{10cm}}}]{%
    Researchers and end-users shall be able to work with the generated data in an efficient manner to produce results quickly. 
    For this purpose, we want to provide simple visualisation tools that help getting an overview about an experimentâ€™s performance, 
    e.g.\@ to illustrate function calling times graphically.
    Further this helps to guide the adoption of our comprehensive framework.
  }\\
  \midrule[0.06em]
  \multicolumn{2}{c}{\emph{\textbf{Non-Functional Requirements}}}\\
  \midrule[0.06em]
  \makecell[{{p{4cm}}}]{%
  Automated Build and Deployment Process %%% Note: Once set up
  }&
  \makecell[{{p{10cm}}}]{%
    Once set up and configured, no manual intervention should be needed 
    to get a program and its workload run.
    This is also crucial for achieving experiment reproducibility, 
    since there are no changeable variables during the build and deployment phases.
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  Minimal Experiment Configuration %%% One File, declarative deployment, simple policy changes
  }&
  \makecell[{{p{10cm}}}]{%
    An experiment should be deterministically described by a single comprehensible configuration file.
    It should be deployed in a declarative way, such that for a simple policy changes
    like moving a function between platforms one only single configuration edits are necessary
    while all remaining work is abstracted.
  }\\
  \bottomrule
\end{longtable}

\begin{longtable}{l l} 
  \caption[Benchmarking Suite Accomplishments]{Our FaaS Benchmarking Suite's Accomplishments\vspace*{1mm}}\label{tab:fulfillments}\\
  \textbf{Requirement} & \textbf{Completion Strategy}\\ 
  \toprule
  %\endhead{}%
  \multicolumn{2}{c}{\emph{\textbf{Functional Requirements}}}\\
  \midrule[0.06em]
  \makecell[{{p{4cm}}}]{%
  Application-Driven Benchmarking Framework
  }&
  \makecell[{{p{10cm}}}]{%
    Our whole framework is based on being application-driven.
    It is possible to develop new applications using our framework which are then
    instrumented and measured automatically (\Cref{sec:jsFunctions}).
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  Representative Demo Applications
  }&
  \makecell[{{p{10cm}}}]{%
    We developed two fully working demo applications:
    A common webshop representing a request-response based application (\Cref{sec:webshop})
    and an event-based IoT data processing application (\Cref{sec:iot}).
    There are various pre-defined workload profiles available for both.
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  Support for Cross-Provider Setups
  }&
  \makecell[{{p{10cm}}}]{%
    Our library's own RPC system secures that functions can be called and measured
    disregarding to which platform they have been deployed (\Cref{sub:designFunctionHandler}).
    Even distributed deployment is done in an automatic way,
    the provider of each function only has to be declared within an experiment's config file (\Cref{sec:experimentConfig}).
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  Easy Adaptibility to New Providers
  }&
  \makecell[{{p{10cm}}}]{%
    Our code is structured so that new providers can be added. 
    We showed this e.g.\@ by fully integrating support for MCC's 
    novel `tinyFaaS' platform (\Cref{sec:providersetuptinyfaas}).
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  Time Measurements %%% Note: Function and Calling
  }&
  \makecell[{{p{10cm}}}]{%
    Whenever one of our functions is run or calls another one, 
    time measurements are taken automatically (\Cref{sub:runtimeProbes,sub:callingTimeProbes}).
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  Request Tracing 
  }&
  \makecell[{{p{10cm}}}]{%
    Whenever one of our functions is called, 
    we inject call tracing probes (\Cref{sub:callTracingProbes}).
    This way, the whole application's internal call tree can be reconstructed after an experiment has run.
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  Coldstart Detection %%% 
  }&
  \makecell[{{p{10cm}}}]{%
    We inject cold-start detection probes into each function (\Cref{sub:coldStartProbing}).
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  External Database Support
  }&
  \makecell[{{p{10cm}}}]{%
    Each of our applications has access to a redis database (\Cref{sub:functionDBAccess})
    which can be declared in an experiment's configuration and 
    is then automatically deployed together with the functions.
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  Data Export to Common Format (CSV) %%% 
  }&
  \makecell[{{p{10cm}}}]{%
    After an experiment has run, its measurement data can be fetched and exported as CSV (\Cref{sub:analysisUsageCSVExport})
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  Simple Data Visualisation %%% Common interests like function time + call graph, technically optional
  }&
  \makecell[{{p{10cm}}}]{%
    Our analysis provide various standard visualisation features, 
  e.g.\@ plotting function execution times and call graphs (\Cref{sec:analysisBasicUsage}).
  }\\
  \midrule[0.06em]
  \multicolumn{2}{c}{\emph{\textbf{Non-Functional Requirements}}}\\
  \midrule[0.06em]
  \makecell[{{p{4cm}}}]{%
  Automated Build and Deployment Process 
  }&
  \makecell[{{p{10cm}}}]{%
    An experiment is automatically built and deployed by executing one command each 
    (\Cref{sec:build,sec:deploy}).
  }\\
  \midrule[0.02em]
  \makecell[{{p{4cm}}}]{%
  Minimal Experiment Configuration %%% One File, declarative deployment, simple policy changes
  }&
  \makecell[{{p{10cm}}}]{%
    All deployment configuration is done within a single well-structured \texttt{experiment.json} configuration file
    (\Cref{sec:experimentConfig}).
    Technically, each applied workload profile has another individual configuration file,
    but they are also referenced (and thereby selected) via the \texttt{experiment.json}.
  }\\
  \bottomrule
\end{longtable}

\subsection{Background and current Landscape}\label{sec:background}

%Serverless cloud computing platforms (FaaS) such as AWS Lambda, Google Cloud Functions or Azure Functions 
%have garnered considerable commercial and scientific interest through the promise of removing deployment concerns from the developer. 
The~FaaS platform-centric approach provides considerable challenges in evaluating performance behavior between different workloads and platforms. 
Thus far there is no single established approach for designing FaaS application benchmarks or an established set of metrics 
to evaluate characteristics such as performance or reliability.

Previous contributions have established approaches for analyzing across multiple platforms~\cite{malawski_benchmarking_2018}, 
big-data workloads~\cite{kuhlenkamp_evaluation_2019} and system characteristics such as platform elasticity~\cite{kuhlenkamp_benchmarking_2020}. 
While the number of contributions has risen considerably over the last few years, 
a~review by \textcite{kuhlenkamp_benchmarking_2018} identified a number of challenges both in regards to workload and problem formulation, 
as well as metrics analyzed, that have still not been sufficiently evaluated.

While recent works~\cites{kuhlenkamp_benchmarking_2020,van_eyk_beyond_2020} have made considerable strides towards addressing these issues, 
a~framework for both building and evaluating performance in FaaS-platforms has still not yet been established.

\subsection{Document Structure}%
\label{sub:introDocStructure}

This document is split into four main parts: 

Its first part covers the theoretical aspects and considerations which formed the framework (\Cref{chap:DesignPrinciples}).
The second part explains our two demo applications, which were build around that framework and can be used for FaaS benchmarking
(\Cref{chap:DemoApplications}).
Afterwards, there is a chapter dedicated to the theoretical foundation of our data analysis tools (\Cref{chap:analysis}).
The last part is a practical usage guide, which explains in detail how to setup and use this project,
including data export and data analysis.
For people that are only interested in using the software it is thus sufficient to read \Cref{chap:UsageGuide}.



\end{document}

